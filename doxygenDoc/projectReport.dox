/*!

\page report Project Report

\author		Jie Jiang
\date		April 26, 2015

\section concept Concept

### Objective

The goal of this project is to build a rendering software that could be able to load a huge amount of data and render it at an interactable frame rate.  

We are really focusing on doing 2 things:

- Load big chunk of data into memory
- Render image as fast as possible

Here's an animation generated from our application:

<img src="../../results/demo_super_fast.gif">

### Design

We designed this software to visualize a large particle data. Assuming that the raw data comes from a set of SDF files. 

The pipeline for our visualization framework is illustrated in the following diagram.

<img src="../../results/concept.JPG">

- Parallel data I/O nodes

	- Loads data in parallel. Each I/O nodes will read the particle data, convert it into a volumetric data and send it to corresponding rendering nodes. 
	Note that the output of each IO node is not to be designated to a certain rendering nodes, but distribute a subvolume of its data to corresponding rendering ndoes.  

- Parallel rendering nodes

	- Applying 3d wavelet transform to its local block of volume data in order to provide level of detail.
	- Rendering outcome images on local volume data.
	- Using binary-swap composite to get the final output image.
	- Sending the pixel data to corresponding displaying nodes. 
	
- Displaying nodes

	- The rendered image could be sent out as one image file for single displaying unit or multiple sub images for a tiled display.

\section tasks Tasks

### Parallel Data Input

The root processor will scan all data file inside the data directory. Then each file will be assigned to one PE only. However, each PE could have multiple files.
Reading the SDF file using libSDF gives us the capability of a page-caching data retrieving. As long as we are sending the data out from the IO nodes before reading more data in, we would never run out of memory even when working with huge SDF data file.  

For situation where there's only one large SDF file instead of multiple smaller files, we could use MPI_IO for simultaneous data reading. However, we haven't tried it yet.

### Particle To Regular Grid

2 steps are needed to convert particle data into a regular grid data:

- determine size of each cell in the grid 
	
	- Traverse the particle data and find out the bounding volume
	- Devide the total length in each direction of the bounding volume to the coresponding grid dimension will give you the size of each cell    

- accumulate each particle to the 8 vertices, which formed the cell of the grid that the particle lies within
	
	- Assign the weight ( assumed to be 1 unless specified other wise ) of the particle to 8 neighboring vertices in the regular grid using interpolation according to the distance between each pair.

The process is shown in this diagram:	
<img src="../../results/particle2volume.jpg">

### Wavelet Transform

- What is a Wavelet Transform?

	- I'm not going to copy&paste what they have <a href="http://en.wikipedia.org/wiki/Wavelet_transform">here</a>, <a href="http://zone.ni.com/reference/en-XX/help/371419D-01/lvasptconcepts/wa_dwt/">here</a> and <a href="http://eeweb.poly.edu/iselesni/WaveletSoftware/standard3D.html">here</a> 
	- A very quick summary is, doing a discrete wavelet transform on 1 dimension of your data will split your data into 2 parts: one containing lower frequency ( approximation ) and the other containing higher frequency ( details ). While the whole size of the data remains unchanged ( Because of the down sample after the convolution ). And after an Inverse DWT we could restore the original data.
	<img src="../../results/wavelet.JPG">
	
- How can it be used here?

	- What would DWT give us? Let look at an image for some ideas. For the image down here, you will find out that the Low frequency part after the DWT contains most of the information from the original picture, while the size has been reduced to 1/16.
	<img src="../../results/2dDWT.png">
	
	- Multi-dimensional DWT is basically applying DWT on each dimension consequtively. For the diagram below, we first apply DWT on x-dimension. Then we have 2 component, the lower frequency Wl and high frequency Wh. Then we apply the DWT to y-dimension to both. So on and so forth. 
	<img src="../../results/3dDWT.jpg">
	<img src="../../results/3dDWT2.jpg">
	
	- Multi-level DWT is achieved by applying another DWT on the LOWER frequency part again. So from the diagram above, we could apply another 3D DWT to WLLL.

	- What does it looks like with our data? The .gif below is composed of 3 rendered image from our application of a 256^3 volume data at detail level of 0 ( original grid of size 256^3 ), 1 ( grid of size 128^3 ) and 2 ( grid of size 64^3 ). Pay some attention to how the major feature has been retained while the resolution has compromised.
	<img src="../../results/test.gif">
	
	- Is it really that good? OK, we all agree that the reduced size would help. But how much help is the filtering doing there? Here's a comparison:
		- for DWT level 1 you can see most informations on the black background has been kept. While the down sample image hasa been suffered lots of information loss on region with sparse data.
		- for the next level, neither case could remain a decent data, but DWT is slightly better.
	
	<table><tr><td><img src="../../results/original.bmp"/></td><td>
	<img src="../../results/level1.bmp"/>
	</td><td>
	<img src="../../results/level2.bmp"/>
	</td></tr>
	<tr><td>
	Original data of 256^3 grid
	</td>
	<td>
	level 1 DWT of 256^3 grid
	</td><td>
	level 2 DWT of 256^3 grid
	</td></tr>
	<tr><td/><td>
	<img  src="../../results/down1sample.bmp"/>
	</td><td>
	<img  src="../../results/down2sample.bmp"/>
	</td></tr>
	<tr><td/><td>
	128^3 grid
	</td><td>
	64^3 grid
	</td></tr>
	</table>

	
### Parallel Volume Rendering

- Volume ray casting 
	
	We are using volume ray casting technique for our volume rendering. We used a orthogonal camera. From 3 vectors, i.e. position, direction and camera up orientation, we could locate the camera in the volume coordinate system. And then create a 2d array of the ray casting from that camera plane.
	Using a similar technique, we could render the scene graph for display setups with specific layout.
	
	For the cylindrical tiled display of CAVE2, we could use two parameters, one being the raster angle and the other one being the height. Then we could generate the ray like:
	
		float3 dx = -cameraDir;
		float3 dy = cameraDir.cross(cameraUpDir);
		for ( int i=30; i<=350; i+=angleStep )
		{	float3 newRayDir = dx + dy * tan(i/360*pi); 
			for ( int h=0; h<= height; h+=heightStep )
			{	float3 newCameraPos = cameraPos + newRayDir * closeClippingPlane;
				rayMarching ( newCameraPos, newRayDir );
			}
		}
		
	<img  src="../../results/cave2.jpg"/> 	

- Binary swap composition

	- <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.1425&rep=rep1&type=pdf">here</a> is the link to the paper explaining sort-last binary-swap composition.

	- Basically we're splitting the data on X-Y-Z-X-... dimension in half each time to form the data blocks. And for the compositing phase, we always communicate with the closer ones first ( since the mask goes from the lowest bit to the highest bit ), so for our composition, the sequence of the propagation on the ray has not been changed.
	
	- Argonne also has a very nice <a href="http://www.mcs.anl.gov/~tpeterka/composite.html">webpage explaining and comparing different composition schemes</a>.

\section challenge Unexpected Challenges

### Ghost cells

- Ghost cells are used a lot when parallel volume rendering is used. Actually it's used all the time whenever there's data parallelism and stencil program. <a href="http://people.csail.mit.edu/fred/ghost_cell.pdf"> Check this out! </a>

- Here's why we needed ghost ceels. Because we split data into different PE. Without the information from the neighboring PE, neither node can interpolate the sample point that lies between the boundary. Hence there will be a missing value. 

<img  src="../../results/ghostCell.jpg"/>

- The traditional way to solve this problem is to keep a padding of one single layer of data from all it's neiboring node. And then each node could interpolate all data between the gap. Problem solved!

- However, applying DWT changes it a little bit. Because each level of DWT would cut data length in half. If we still want to have that single layer of padding from neighboring cell, we could 

	- Keep a layer of power(2,maxLevel) data padding for each neighboring node
	- or, access transformed discrete wavelet coefficients from neighboring node during run time
	
	This is a trade off between the memory and the run time. Decision should be picked on a case by case basis. Unfortunately, until this point, I have not implemented it in our application yet.   

\section software Software & Tools

- github project <a href="https://github.com/jjsjann123/cosmology">link</a>;
- libSDF is used for reading particle information from each file;
- mpich is used for communication;
- EasyBMP is used to output rendered image into bmp files;
- WaveletAnalysis is used for our multi-dimensional wavelet transform;
- Doxygen is used to generate the documentation;
- Git is used for version control;

 */
