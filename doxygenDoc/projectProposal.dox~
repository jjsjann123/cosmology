/*!

\page proposal Project Proposal

\author		Jie Jiang
\date		March 19, 2015

\section intro Introduction

### Cosmology Application

Cosmological simulations are vital of understanding the Universe's development over a 13.7 billion year's time and how the galaxies and clusters of galaxies surrounding us evolves. The mass-energy of the known universe is composed of 4.9% ordinary matter (Atoms), 26.8% <a href="http://en.wikipedia.org/wiki/Dark_matter">dark matter</a> and 68.3% <a href="http://en.wikipedia.org/wiki/Dark_energy">dark energy</a>. Because dark energy plus dark matter constitute 95.1% of the total mass-energy content of the universe. A theorist's universe has dark matter only, according to Katrin Heitmann from Los Alamos National Laboratory.

<a href="http://darksky.slac.stanford.edu/">Dark Sky Simulations</a> are an ongoing series of cosmological N-body simulations designed to provide quantitative model of the large-scale simulation data, as well as analysis tools, to the public. 

For SciVis2015 contest, dataset from the ds14 Simulations have been used.

> ds14_a is the first trillion particle simulation released to the public, and is being used to study the very largest scales in the Universe. It spans a cubical region 8 Gpc/h on a side, nearly 40 billion light years across. With 102403 (~1.07 trillion) particles, we follow the growth of the largest clusters of galaxies, weighing in at more than a quadrillion times the mass of our own sun.
> This simulation, as well as a set of lower-particle count simulations (20483-40963), are referred to as the ds14_g simulations, and are the first in a set of simulations enabled through DOE INCITE computing grant project titled Probing Dark Matter at Extreme Scales. of 80 million CPU hours on the Oak Ridge National Laboratory Titan Supercomputer. 

### Task

There are multiple tasks are required in the visualization by the contest. In this project, we'll focus on one thing:

- Initial data integration and browsing

### Scope

This scope of this project includes:

- Parallel data read
- Capabilities for navigation and interaction
- Particle data rendering with progressive data access

\section data Data

### Datasets

There are three primary types of data from the contest. The raw particle data is generated by cosmological N-body simulation. Then a <a href="https://bitbucket.org/darkskysims/rockstar">Rockstar Halo Finder</a> was used to extract halo and merger tree from the particl data set. Rockstar Halo Finder has an open source and could be extended with more input/output formats and properties. It is also possible, in fact strongly recommended by the developer, to include more sophisticated package to find descendant information for each halo for science purposes. 

- Raw particle data

	Formated in Self-Describing Files (<a href="https://bitbucket.org/JohnSalmon/sdf">SDF</a>) that is composed of a ASCII header followed by raw binary data.

	Each particle contains multiple field. And there's
	+ position vector
	+ velocity vector
	+ unique particle identifier

	The header structure for corresponding fields are defined as:

		struct {
			float x, y, z;              /* position of body */
			float vx, vy, vz;           /* velocity of body */
			float ax, ay, az;           /* acceleration */
			float phi;                  /* potential */
			int64_t ident;              /* unique identifier */
		};

- Halo catalog

	Defines a database that groups sets of gravitationally bound particles together into coherent structures ( <a href="http://en.wikipedia.org/wiki/Dark_matter_halo">Dark Matter Halo</a> ) along with information like
	+ position
	+ shape
	+ size
	+ other statistics

- Merger tree

	Merger tree database is created via inking individual halo catalogs in each snapshot in time. 


For our project, we will be using only the raw particle data. We will convert Unstructured grid data (particle) into structured regular grid data. 

Converting particle dataset into volumetric dataset, we could use volume rendering for our visualization. Moreover, it then becomes possible to use wavelet transform based level of detail (LOD) to speed up the response of the application while navigating through the dataset. 

\section tasks Task(s)

The project is intended to build an interactive visualization application of the dark sky simulation data.

Rendering process will be running on a cluster. 

-# We split and distribute the raw data in a set of files to specific rendering node. A parallel data reader loads the particle data on each node.
-# Rendering nodes then map the particle data into 8 grid points of the cell that it falls into. This creates a regular grid data on each rendering node.
-# We run a wavelet transform on each grid data. The output will a grid data with reduced size and series of complementary data of different level of detail.
-# A parallel volumetric rendering algorithm will be used to render an image according to current camera information.

Aside from the rendering work flow, the application provides basic interaction for the user the navigate the dataset. The user could be able to fly through the application via keyboard commands on the master node.

After the camera has been moved, rendering nodes start rendering the image from the coarsest data set. The idea is to reduce the size of the data set in order to boost the rendering process. Hence it maintains a stable frame rate. After the camera position is fixed, we apply inverse wavelet transform for each following frame and gradually restore the resolution of the original data. So more details would be available to the user.

\section software Software

Several libraries will be leveraged in our application.

- libSDF will be used for reading particle information from each file;
- OpenMP will be used for communication between rendering nodes;
- vtk for graphics rendering

Other library might also be considered as alternatives:
- We might be able to re-use some Paraview framework to have rendered images streaming to a remote desktop
- If running on CAVE2 at UIC. Omegalib could be utilized hence provide a VR environment for data exploration.

\section design Visualization Design

Because blue water cluster does not provide any accessible display, we plan to use a remote displaying device to receive the image/scene from the rendering cluster. <a href="https://www.evl.uic.edu/entry.php?id=2016">CAVE2</a> at Electronic Visualization Lab from UIC is going to be used as the remote visualization cluster. Two proposed visualization designs are:

- CAVE2 rendering + visualization

- Remote rendering + CAVE2 visualization

The implementation of our visualization includes 2 phases. Initially we will use CAVE2 as rendering nodes as well as visualization nodes. The idea of this design is to eliminate the complexity resulted from communication interruption. Later we will try to run rendering on a remote cluster ( most likely blue water ), and stream the rendered image back into CAVE2.

\section expection Expected Results and Evaluation

- Preliminary work

	We have downloaded the ds14_scivis_0128 data package from the contest website. We need only the particle data set in SDF format. Most likely we will distrubte only the corresponding files to the cluster.

	For better load balance while rendering the data, Pre-processing might be necessary for data files. Re-group each particle data according to its position vector.

- Expected result

	- The application could load all particle data from the ds14_scivis_0128 in parallel and convert it into a volumetric data set at each nodes. 
	- Rendering nodes could redistribute the volumetric data set and composite the parallel volume rendering cache.
	- For a given data set, the application could apply a series of wavelet transform to obtain different level of detail.
	- User will be able to update the camera information and rendering snapshot to explore the data structure. Level of detail would automatically adjust while the user is updating the camera position or snapshot.

 */
